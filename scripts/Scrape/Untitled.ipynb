{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = \"../assignment2dataset/train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeTrainSet = []\n",
    "with open(trainPath) as fp: \n",
    "    Lines = fp.readlines() \n",
    "    sentence = []\n",
    "    for line in Lines: \n",
    "        lineList = line.split(\" \")\n",
    "        word = lineList[0]\n",
    "        if word==\"\\n\":\n",
    "            wholeTrainSet.append(sentence)\n",
    "            sentence = []\n",
    "            continue\n",
    "        posTag = lineList[1]\n",
    "        chunkLabel = lineList[2][0]\n",
    "        sentence.append((word,posTag,chunkLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixesList = [\"under\", \"fore\", \"mid\", \"mis\", \"over\", \"auto\",\"super\",\"up\",\"uni\",\"un\",\"tri\",\"trans\",\"tele\",\"sym\",\"syn\",\"sub\",\"pre\",\"pro\",\"post\",\"omni\",\"non\",\"mono\",\n",
    "                \"micro\",\"macro\",\"intro\",\n",
    "                \"intra\",\"inter\", \"in\", \"il\", \"im\", \"ir\", \"hyper\", \"homo\", \"homeo\", \"hetero\", \"extra\", \"ex\", \"en\",\n",
    "                \"dis\", \"de\", \"contra\", \"contro\", \"com\", \"con\", \"co\", \"circum\", \"auto\", \"anti\", \"ante\", \"an\"\n",
    "               ]\n",
    "prefixesList = sorted(prefixesList,key= lambda word : (-len(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixList = []\n",
    "suffixPath = \"./suffix.txt\"\n",
    "with open(suffixPath) as fp: \n",
    "    Lines = fp.readlines() \n",
    "    for line in Lines: \n",
    "        lineList = line.split(\"\\n\")\n",
    "        suffixList.append(lineList[0].lower())\n",
    "suffixes = list(set(suffixList))\n",
    "suffixList = sorted(suffixList,key= lambda word : (-len(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defaultFeatureValues() :\n",
    "    features = {}\n",
    "    for i in prefixesList :\n",
    "        features[i] = False\n",
    "    for i in suffixList :\n",
    "        features[i] = False \n",
    "    features['prev_prev_word'] = \"NA\"\n",
    "    features['prev_word'] = \"NA\"\n",
    "    features['word'] = \"NA\"\n",
    "    features['next_word'] = \"NA\"\n",
    "    features['next_next_word'] = \"NA\"\n",
    "    features['capitalization'] = False\n",
    "    features['start_of_sentence'] = False\n",
    "    features['prev_prev_word_chunk'] = \"NA\"\n",
    "    features['prev_word_chunk'] = \"NA\"\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectPrefix(word):\n",
    "    for prefix in prefixesList:\n",
    "        if word.lower().startswith(prefix) and len(word) - len(prefix) > 1:\n",
    "            return(prefix)\n",
    "    return \"NO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectSuffix(word):\n",
    "    for suffix in suffixList:\n",
    "        if word.lower().endswith(suffix) and len(word) - len(suffix) > 1:\n",
    "            return(suffix)\n",
    "    return \"NO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet = []\n",
    "for index,sentence in enumerate(wholeTrainSet) :\n",
    "    if len(sentence) < 5 :\n",
    "        continue\n",
    "    for i in range(len(sentence)):\n",
    "        word = sentence[i]\n",
    "        features = defaultFeatureValues()\n",
    "        prefix = detectPrefix(word[0])\n",
    "        suffix = detectSuffix(word[0])\n",
    "        if prefix != \"NO\":\n",
    "            features[prefix] = True\n",
    "        if suffix != \"NO\":\n",
    "            features[suffix] = True\n",
    "        features['capitalization'] = word[0][0].isupper()\n",
    "        features['word'] = word[0]\n",
    "        if i == 0 :\n",
    "            features['start_of_sentence'] = True\n",
    "            features['next_word'] = sentence[i+1][0]\n",
    "            features['next_next_word'] = sentence[i+2][0]\n",
    "        elif i == 1 :\n",
    "            features['next_word'] = sentence[i+1][0]\n",
    "            features['next_next_word'] = sentence[i+2][0]\n",
    "            features['prev_word'] = sentence[i-1][0]\n",
    "            features['prev_word_chunk'] = sentence[i-1][2]\n",
    "        elif i == len(sentence) - 2 :\n",
    "            features['prev_prev_word'] = sentence[i-2][0]\n",
    "            features['prev_prev_word_chunk'] = sentence[i-2][2]\n",
    "            features['prev_word_chunk'] = sentence[i-1][2]\n",
    "            features['prev_word'] = sentence[i-1][0]\n",
    "            features['next_word'] = sentence[i+1][0]\n",
    "        elif i == len(sentence) - 1 :\n",
    "            features['prev_prev_word'] = sentence[i-2][0]\n",
    "            features['prev_prev_word_chunk'] = sentence[i-2][2]\n",
    "            features['prev_word_chunk'] = sentence[i-1][2]\n",
    "            features['prev_word'] = sentence[i-1][0]\n",
    "        else :\n",
    "            features['prev_prev_word'] = sentence[i-2][0]\n",
    "            features['prev_prev_word_chunk'] = sentence[i-2][2]\n",
    "            features['prev_word_chunk'] = sentence[i-1][2]\n",
    "            features['prev_word'] = sentence[i-1][0]\n",
    "            features['next_word'] = sentence[i+1][0]\n",
    "            features['next_next_word'] = sentence[i+2][0]\n",
    "        TrainSet.append((features,word[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"./my_classifier_withoutPOS.pickle\"):\n",
    "    maxent_classifier = nltk.classify.MaxentClassifier.train(TrainSet, max_iter=30)\n",
    "    f = open(\"my_classifier_withoutPOS.pickle\", \"wb\")\n",
    "    pickle.dump(maxent_classifier , f)\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPath = \"../assignment2dataset/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeTestSet = []\n",
    "with open(testPath) as fp: \n",
    "    Lines = fp.readlines() \n",
    "    sentence = []\n",
    "    for line in Lines: \n",
    "        lineList = line.split(\" \")\n",
    "        word = lineList[0]\n",
    "        if word==\"\\n\":\n",
    "            wholeTestSet.append(sentence)\n",
    "            sentence = []\n",
    "            continue\n",
    "        posTag = lineList[1]\n",
    "        tag = lineList[2][0]\n",
    "        sentence.append((word,posTag,tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSet = [] \n",
    "TestLabel = []\n",
    "for index,sentence in enumerate(wholeTestSet) :\n",
    "    if len(sentence) < 5 :\n",
    "        continue\n",
    "    for i in range(len(sentence)):\n",
    "        word = sentence[i]\n",
    "        features = defaultFeatureValues()\n",
    "        prefix = detectPrefix(word[0])\n",
    "        suffix = detectSuffix(word[0])\n",
    "        if prefix != \"NO\":\n",
    "            features[prefix] = True\n",
    "        if suffix != \"NO\":\n",
    "            features[suffix] = True\n",
    "        features['capitalization'] = word[0][0].isupper()\n",
    "        features['word'] = word[0]\n",
    "        features['word_tag'] = word[1]\n",
    "        if i == 0 :\n",
    "            features['start_of_sentence'] = True\n",
    "            features['next_word'] = sentence[i+1][0]\n",
    "            features['next_next_word'] = sentence[i+2][0]\n",
    "            features['next_word_tag'] = sentence[i+1][1]\n",
    "            features['next_next_word_tag'] = sentence[i+2][1]\n",
    "        elif i == 1 :\n",
    "            features['next_word'] = sentence[i+1][0]\n",
    "            features['next_next_word'] = sentence[i+2][0]\n",
    "            features['next_word_tag'] = sentence[i+1][1]\n",
    "            features['next_next_word_tag'] = sentence[i+2][1]\n",
    "            features['prev_word'] = sentence[i-1][0]\n",
    "            features['prev_word_tag'] = sentence[i-1][1]\n",
    "            features['prev_word_chunk'] = sentence[i-1][2]\n",
    "        elif i == len(sentence) - 2 :\n",
    "            features['prev_prev_word'] = sentence[i-2][0]\n",
    "            features['prev_prev_word_tag'] = sentence[i-2][1]\n",
    "            features['prev_prev_word_chunk'] = sentence[i-2][2]\n",
    "            features['prev_word_chunk'] = sentence[i-1][2]\n",
    "            features['prev_word'] = sentence[i-1][0]\n",
    "            features['prev_word_tag'] = sentence[i-1][1]\n",
    "            features['next_word'] = sentence[i+1][0]\n",
    "            features['next_word_tag'] = sentence[i+1][1]\n",
    "        elif i == len(sentence) - 1 :\n",
    "            features['prev_prev_word'] = sentence[i-2][0]\n",
    "            features['prev_prev_word_tag'] = sentence[i-2][1]\n",
    "            features['prev_prev_word_chunk'] = sentence[i-2][2]\n",
    "            features['prev_word_chunk'] = sentence[i-1][2]\n",
    "            features['prev_word'] = sentence[i-1][0]\n",
    "            features['prev_word_tag'] = sentence[i-1][1]\n",
    "        else :\n",
    "            features['prev_prev_word'] = sentence[i-2][0]\n",
    "            features['prev_prev_word_tag'] = sentence[i-2][1]\n",
    "            features['prev_prev_word_chunk'] = sentence[i-2][2]\n",
    "            features['prev_word_chunk'] = sentence[i-1][2]\n",
    "            features['prev_word'] = sentence[i-1][0]\n",
    "            features['prev_word_tag'] = sentence[i-1][1]\n",
    "            features['next_word'] = sentence[i+1][0]\n",
    "            features['next_word_tag'] = sentence[i+1][1]\n",
    "            features['next_next_word'] = sentence[i+2][0]\n",
    "            features['next_next_word_tag'] = sentence[i+2][1]\n",
    "        TestLabel.append(word[2])\n",
    "        TestSet.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"my_classifier_withoutPOS.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [loaded_model.classify(i) for i in TestSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8493738894999577, 0.8493738894999577, 0.8493738894999577, None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(TestLabel, predicted,average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8379230751651164, 0.891723617968237, 0.8639866154534958, None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(TestLabel, predicted,labels=['B','I'],average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22925,   871,    12],\n",
       "       [ 3552, 13740,    17],\n",
       "       [ 1376,  1293,  3490]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(TestLabel, predicted,labels=['B','I','O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
